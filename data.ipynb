{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 패션 이미지 데이터셋 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# path = 'data/meta/'\n",
    "# train = pd.read_json(path + 'train_no_dup.json')\n",
    "# valid = pd.read_json(path + 'valid_no_dup.json')\n",
    "# test = pd.read_json(path + 'test_no_dup.json')\n",
    "# blank = pd.read_json(path + 'fill_in_blank_test.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dataloader.sampling import DataSampler\n",
    "#from dataloader.multimodal_data import MultiModalData\n",
    "from dataloader.multilstm_data import MultiModalData\n",
    "import os\n",
    "\n",
    "base_dir = os.getcwd()\n",
    "data_dir = os.path.join(base_dir, 'data')\n",
    "meta_dir = os.path.join(data_dir, 'meta')\n",
    "image_dir = os.path.join(data_dir, 'images')\n",
    "sampler = DataSampler(data_path = meta_dir, k=50, test_sampling_ratio=1)\n",
    "concat_df, question_data = sampler.sample_data()\n",
    "\n",
    "train_dataset = MultiModalData(concat_df, sampler.category_df, image_dir, mode='train')\n",
    "valid_dataset = MultiModalData(concat_df, sampler.category_df, image_dir, mode='valid')\n",
    "test_dataset = MultiModalData(concat_df, sampler.category_df, image_dir, question = question_data, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>items</th>\n",
       "      <th>desc</th>\n",
       "      <th>set_id</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>School</td>\n",
       "      <td>[{'index': 1, 'name': 'colors crop top tee', '...</td>\n",
       "      <td>#red #outfit #beautiful #converse #geek #school</td>\n",
       "      <td>100119331</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fashion Polyvore!</td>\n",
       "      <td>[{'index': 1, 'name': 'shein sheinside black l...</td>\n",
       "      <td>A fashion look from October 2014 featuring cot...</td>\n",
       "      <td>136372956</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>red &amp; pink</td>\n",
       "      <td>[{'index': 1, 'name': 'maya maxi dress', 'pric...</td>\n",
       "      <td>A fashion look from June 2016 by lisamichele-c...</td>\n",
       "      <td>200277215</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cool chic</td>\n",
       "      <td>[{'index': 1, 'name': 'black short sleeve lace...</td>\n",
       "      <td>http://www.romwe.com/Black-Faux-Suede-Button-F...</td>\n",
       "      <td>206996085</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Different formal style</td>\n",
       "      <td>[{'index': 1, 'name': 'james contrast panel sh...</td>\n",
       "      <td>A fashion look from November 2014 featuring co...</td>\n",
       "      <td>141781954</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Perfect Puffer Jackets</td>\n",
       "      <td>[{'index': 1, 'name': 'mcq alexander mcqueen w...</td>\n",
       "      <td>#puffers</td>\n",
       "      <td>214379138</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Sweet Jeans!</td>\n",
       "      <td>[{'index': 1, 'name': 'rails long sleeve arrow...</td>\n",
       "      <td>A fashion look from January 2017 by agnesmakon...</td>\n",
       "      <td>214877329</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>School party with Leo</td>\n",
       "      <td>[{'index': 1, 'name': 'young wild free iphone ...</td>\n",
       "      <td>A beauty collage from January 2014 featuring e...</td>\n",
       "      <td>110637600</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Three Piece Outfit</td>\n",
       "      <td>[{'index': 1, 'name': 'boutique moschino cardi...</td>\n",
       "      <td>A fashion look from September 2016 by sjlew fe...</td>\n",
       "      <td>207266399</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Date night with Luke</td>\n",
       "      <td>[{'index': 1, 'name': 'light blue lace belted ...</td>\n",
       "      <td>A fashion look from February 2016 by joanaoliv...</td>\n",
       "      <td>189795216</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     name                                              items  \\\n",
       "0                  School  [{'index': 1, 'name': 'colors crop top tee', '...   \n",
       "1       Fashion Polyvore!  [{'index': 1, 'name': 'shein sheinside black l...   \n",
       "2              red & pink  [{'index': 1, 'name': 'maya maxi dress', 'pric...   \n",
       "3               Cool chic  [{'index': 1, 'name': 'black short sleeve lace...   \n",
       "4  Different formal style  [{'index': 1, 'name': 'james contrast panel sh...   \n",
       "5  Perfect Puffer Jackets  [{'index': 1, 'name': 'mcq alexander mcqueen w...   \n",
       "6            Sweet Jeans!  [{'index': 1, 'name': 'rails long sleeve arrow...   \n",
       "7   School party with Leo  [{'index': 1, 'name': 'young wild free iphone ...   \n",
       "8      Three Piece Outfit  [{'index': 1, 'name': 'boutique moschino cardi...   \n",
       "9    Date night with Luke  [{'index': 1, 'name': 'light blue lace belted ...   \n",
       "\n",
       "                                                desc     set_id   type  \n",
       "0    #red #outfit #beautiful #converse #geek #school  100119331  train  \n",
       "1  A fashion look from October 2014 featuring cot...  136372956  train  \n",
       "2  A fashion look from June 2016 by lisamichele-c...  200277215  train  \n",
       "3  http://www.romwe.com/Black-Faux-Suede-Button-F...  206996085  train  \n",
       "4  A fashion look from November 2014 featuring co...  141781954  train  \n",
       "5                                           #puffers  214379138  train  \n",
       "6  A fashion look from January 2017 by agnesmakon...  214877329  train  \n",
       "7  A beauty collage from January 2014 featuring e...  110637600  train  \n",
       "8  A fashion look from September 2016 by sjlew fe...  207266399  train  \n",
       "9  A fashion look from February 2016 by joanaoliv...  189795216  train  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['texts', 'images', 'set_id'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[10].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'texts': tensor([[ 0.1392, -0.0566,  0.0559,  ...,  0.2035,  0.2490,  0.2822],\n",
       "         [ 0.0786,  0.3379,  0.2666,  ...,  0.2688,  0.1572, -0.1230],\n",
       "         [ 0.3015,  0.1747, -0.2034,  ...,  0.2964, -0.0151, -0.0194],\n",
       "         [ 0.1310, -0.1548,  0.1309,  ...,  0.4358,  0.0953, -0.0227],\n",
       "         [ 0.4648,  0.0629, -0.4102,  ...,  0.8442,  0.1186,  0.4973],\n",
       "         [-0.0402,  0.0879, -0.1364,  ...,  0.3044,  0.3669,  0.1902]],\n",
       "        device='cuda:0', grad_fn=<ToCopyBackward0>),\n",
       " 'images': tensor([[ 0.0453, -0.2100,  0.2167,  ...,  0.5605, -0.0876,  0.6680],\n",
       "         [-0.1577,  0.3599,  0.1707,  ...,  0.4045,  0.0636,  0.6235],\n",
       "         [-0.0429,  0.3560,  0.3254,  ...,  0.8545, -0.0536,  0.0138],\n",
       "         [-0.1941, -0.2360,  0.6460,  ...,  0.6987,  0.2035,  0.1857],\n",
       "         [ 0.0328, -0.1943, -0.1786,  ...,  0.4023,  0.1215,  0.3862],\n",
       "         [ 0.2908,  0.3110, -0.3909,  ...,  0.7476,  0.1606,  0.2140]],\n",
       "        device='cuda:0', grad_fn=<ToCopyBackward0>),\n",
       " 'set_id': 214379138}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "torch.cat(): expected a non-empty list of Tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_dataset)):\n\u001b[1;32m----> 2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mtrain_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\jsj91\\.vscode\\AI_MMR\\dataloader\\multilstm_data.py:67\u001b[0m, in \u001b[0;36mMultiModalData.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     65\u001b[0m text_input \u001b[38;5;241m=\u001b[39m clip\u001b[38;5;241m.\u001b[39mtokenize(text_list)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     66\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mencode_text(text_input)\n\u001b[1;32m---> 67\u001b[0m image_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     68\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mencode_image(image_tensor)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# 이미지 시퀀스 길이 맞춰주기\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jsj91\\.vscode\\AI_MMR\\dataloader\\multilstm_data.py:24\u001b[0m, in \u001b[0;36mMultiModalData.preprocess_images\u001b[1;34m(self, image_list)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_images\u001b[39m(\u001b[38;5;28mself\u001b[39m, image_list): \u001b[38;5;66;03m# for clip preprocess\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     processed_images \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(image)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m image_list]\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: torch.cat(): expected a non-empty list of Tensors"
     ]
    }
   ],
   "source": [
    "for i in range(len(train_dataset)):\n",
    "    print(train_dataset[i]['images'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1749, 305)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(valid_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    배치 내 데이터를 패딩 처리하고 필요한 키로 구성된 dictionary 반환.\n",
    "    \"\"\"\n",
    "    image_embeddings = [item['images'] for item in batch]\n",
    "    seq_embeddings = [item['texts'] for item in batch]\n",
    "    lengths = torch.tensor([len(seq) for seq in image_embeddings])\n",
    "\n",
    "    image_embeddings_padded = pad_sequence(image_embeddings, batch_first=True)\n",
    "    seq_embeddings_padded = pad_sequence(seq_embeddings, batch_first=True)\n",
    "    mask = (image_embeddings_padded.sum(dim=2) != 0).float()\n",
    "\n",
    "    return {\n",
    "        'image_embeddings': image_embeddings_padded,\n",
    "        'seq_embeddings': seq_embeddings_padded,\n",
    "        'lengths': lengths,\n",
    "        'mask': mask\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "#from AI_MMR.model.multilstm import Multifusion, MultiLSTM\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "        train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn\n",
    "    )\n",
    "valid_dataloader = DataLoader(\n",
    "        valid_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "torch.cat(): expected a non-empty list of Tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Dataloader\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mbreak\u001b[39;49;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\jin\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\jin\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\jin\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\jin\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\jsj91\\.vscode\\AI_MMR\\dataloader\\multilstm_data.py:67\u001b[0m, in \u001b[0;36mMultiModalData.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     65\u001b[0m text_input \u001b[38;5;241m=\u001b[39m clip\u001b[38;5;241m.\u001b[39mtokenize(text_list)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     66\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mencode_text(text_input)\n\u001b[1;32m---> 67\u001b[0m image_tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     68\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mencode_image(image_tensor)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# 이미지 시퀀스 길이 맞춰주기\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jsj91\\.vscode\\AI_MMR\\dataloader\\multilstm_data.py:24\u001b[0m, in \u001b[0;36mMultiModalData.preprocess_images\u001b[1;34m(self, image_list)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_images\u001b[39m(\u001b[38;5;28mself\u001b[39m, image_list): \u001b[38;5;66;03m# for clip preprocess\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     processed_images \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(image)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m image_list]\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: torch.cat(): expected a non-empty list of Tensors"
     ]
    }
   ],
   "source": [
    "# Dataloader\n",
    " \n",
    "for batch_idx, batch in enumerate(train_dataloader):\n",
    "    print(batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "for batch_idx, batch in enumerate(valid_dataloader):\n",
    "    print(batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = Multifusion().to(device)\n",
    "lstm = MultiLSTM(input_size=512, hidden_size=512, num_layers=1, bidirectional=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AI_MMR.model.multilstm import Multifusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
